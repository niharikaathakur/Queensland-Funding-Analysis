{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "319899bc-3fc3-4e1a-8c26-2990db3b9c34",
            "metadata": {
                "cell_name": "header_cell"
            },
            "source": [
                "<div style=\"background:#E9FFF6; color:#440404; padding:8px; border-radius: 4px; text-align: center; font-weight: 500;\">IFN619 - Data Analytics for Strategic Decision Makers (2024 Sem 1)</div>"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "bf483340-54bb-4181-86ae-e410f1efbb7a",
            "metadata": {
                "cell_name": "title_cell"
            },
            "source": [
                "# IFN619 :: C2 - Machine Learning\n",
                "\n",
                "Machine Learning is a process of data analysis where the computer 'learns' a model of a particular aspect of the data being analysed. The process involves 'training' whereby a particular type of model is 'fit' to the data, creating a model which can be used to classify or predict from within the same data, or on new data. \n",
                "\n",
                "The success of machine learning typically relies on selecting an appropriate ML approach that suits both the data and the question being answered. For example, the topic modelling that we did in C1 involved the assumption that there were latent topics in the data that could be analysed by fitting a topic model to the data. If this assumption was true, if the data was appropriately structured, and if we selected a reasonable value for 'k' (number of topics), then we were able to obtain helpful topics that described that underlying data. However, if any of these conditions were not met, then the quality of the results deteriorated. Thus, when using ML, it is important to know your data, understand what question/s you are trying to address, know which ML models might be appropriate, and be willing to experiment with different combinations of models and parameters in order to obtain reliable results.\n",
                "\n",
                "To help select an approach, we can answer 2 questions:\n",
                "\n",
                "1. What kind of data?\n",
                "    - LABELLED DATA -> Supervised ML\n",
                "    - UNLABELLED DATA => Unsupervised ML\n",
                "\n",
                "2. What do we want to achieve?\n",
                "    - PREDICT -> Regression algorithms\n",
                "    - LABEL -> Classification algorithms\n",
                "    - GROUP -> Clustering algorithms\n",
                "\n",
                "*Note: The above don't cover all ML, but provide a good starting point for exploration*\n",
                "\n",
                "In this session we are going to look at 3 simple examples:\n",
                "\n",
                "1. Group unlabelled data into clusters (groups) and label new data using a K-means algorithm\n",
                "2. Predict a value for new data based on existing labelled data using Linear Regression\n",
                "3. Predict a label for new data based on existing labelled data using Logistic Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4708f59f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "# Scikit Learn is a python machine learning library see: https://scikit-learn.org/stable/\n",
                "\n",
                "from sklearn.preprocessing import minmax_scale\n",
                "from sklearn.cluster import KMeans\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.metrics import classification_report\n",
                "\n",
                "import pandas as pd\n",
                "import plotly.express as px\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "2965232a",
            "metadata": {},
            "source": [
                "### K-means algorithm\n",
                "\n",
                "K-means is an iterative algorithm that aims to identify centroids (means) in the data for a given number of clusters (k). It then assigns a cluster label (K-value) to each observation in the data based on the nearest centroid (mean). For more details see this [Wikipedia page](https://en.wikipedia.org/wiki/K-means_clustering)\n",
                "\n",
                "![k-means](./graphics/k-means-wikipedia.png)\n",
                "\n",
                "[Image from Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "7f4cab7e",
            "metadata": {},
            "source": [
                "### EXAMPLE - Fair work place\n",
                "\n",
                "As a starting example, we will use 'toy' fair work place data (we will use this again in the C3 ethics module). \n",
                "\n",
                "### Data preparation\n",
                "\n",
                "The first thing we notice is that the data is not all numeric (workers are W, male is M, etc), and that the numerical data is on an arbitrary scale. To ensure that we can cluster using Euclidean distance it is important to normalise the components (put them on the same scale) which typically is 0-1. This means scaling the rating to be between 0 and 1, and converting the text to numbers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "10f55604",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the data\n",
                "\n",
                "file_path = \"data/\"\n",
                "file_name = \"fair-workplace-survey.csv\"\n",
                "fwp_df = pd.read_csv(f\"{file_path}{file_name}\", index_col=\"id\")\n",
                "fwp_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0bbce18c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert Text to numbers\n",
                "\n",
                "conversions = {\"W\":1,\"S\":0,\"F\":1,\"M\":0,\"O\":0.5} # Note that allowing an 'other' value can accommodate future data\n",
                "\n",
                "# Get numeric values for Role\n",
                "fwp_df[\"Role_n\"] = fwp_df['Role'].apply(lambda x: conversions[x])\n",
                "fwp_df\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "462ae416",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get numeric values for gender\n",
                "\n",
                "fwp_df[\"Gender_n\"] = fwp_df['Gender'].apply(lambda x: conversions[x])\n",
                "fwp_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "68dbf58b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scale the fair workplace rating data\n",
                "\n",
                "minmax_scale(fwp_df['FairWorkPlace'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "192f78b4",
            "metadata": {},
            "outputs": [],
            "source": [
                "fwp_df['rating_n'] = minmax_scale(fwp_df['FairWorkPlace'])\n",
                "fwp_df"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "a16b35a4",
            "metadata": {},
            "source": [
                "#### Clustering\n",
                "\n",
                "Now we have numerical data with all components on the same scale, we can perform the clustering.\n",
                "\n",
                "To do so, we need to set the number of clusters we want as these are the number of groups that the algorithm is going to try and fit the data to."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ff7c22a4-d8e9-4247-b870-425f856790f0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# K-means - 2 clusters\n",
                "km2 = KMeans(n_clusters=2, random_state=0).fit(fwp_df[['Role_n','Gender_n','rating_n']])\n",
                "fwp_df['km2'] = km2.labels_\n",
                "fwp_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "53a4e781",
            "metadata": {},
            "outputs": [],
            "source": [
                "# K-means - 3 clusters\n",
                "km3 = KMeans(n_clusters=3, random_state=0).fit(fwp_df[['Role_n','Gender_n','rating_n']])\n",
                "fwp_df['km3'] = km3.labels_\n",
                "fwp_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aedfb6b9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# K-means - 4 clusters\n",
                "km4 = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(fwp_df[['Role_n','Gender_n','rating_n']])\n",
                "fwp_df['km4'] = km4.labels_\n",
                "fwp_df"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "ea503471",
            "metadata": {},
            "source": [
                "#### Visualise the clusters\n",
                "\n",
                "We can now visualise the clusters - this would be particularly helpful if the dataset was large."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "272cd8b6",
            "metadata": {},
            "outputs": [],
            "source": [
                "fig = px.scatter_3d(fwp_df, x='Role_n', y='Gender_n', z='rating_n',\n",
                "              color='km4')\n",
                "fig.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "b32fa402",
            "metadata": {},
            "source": [
                "#### Classify unseen data\n",
                "\n",
                "The model can be used to classify new data that wasn't part of the train/test data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8a4e22ec",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a dataframe with the new data\n",
                "new_df = pd.DataFrame([[1,1,5]],columns=['Role_n','Gender_n','rating_n'])\n",
                "\n",
                "new_df['km4_predict'] = km4.predict(new_df)\n",
                "new_df"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "7d1ed7d2",
            "metadata": {},
            "source": [
                "### Supervised ML - Training and Testing\n",
                "\n",
                "Supervised machine learning algorithms use labeled datasets to predict labels (categorical data) or values (numerical data). As input data is fed into the model, it adjusts its weights until the model has been fitted appropriately. Supervised machine learning algorithms help organizations solve for a variety of real-world problems at scale, such as predicting future income or if a person is going to default a credit.\n",
                "\n",
                "Supervised machine learning uses a portion of the data (training set) to train a given model to yield the given labels/values. The remainder of the data (test set) can be used to test the effectiveness of the model against the provided labels/values. Once the model is yielding reasonable results, it can be used to predict labels/values for unseen data (data that hasn't been labelled).\n",
                "\n",
                "- The training dataset includes inputs and correct outputs, which allow the model to learn over time \n",
                "- The testing dataset includes inputs and correct outputs but the model is only given the inputs. The model is evaluated by comparing its predicted value (model outputs) to the actual value (correct outputs)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "40e2b0d8",
            "metadata": {},
            "source": [
                "### Linear Regression algorithm\n",
                "\n",
                "Linear regression is used to identify the relationship between a dependent variable and one or more independent variables and is typically leveraged to make predictions about future outcomes. The results from the linear regression help in predicting an unknown value depending on the relationship with the predicting variables. Linear regression fits a straight line that minimizes the discrepancies between predicted and actual output values. Further information in [Linear regression](https://en.wikipedia.org/wiki/Linear_regression).\n",
                "\n",
                "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Linear_least_squares_example2.svg/440px-Linear_least_squares_example2.svg.png\" style=\"width:50%\">\n",
                "\n",
                "#### Data\n",
                "\n",
                "Load and get to know the data.\n",
                "\n",
                "The included dataset is derived from a [dataset of realestate property prices in Darwin](https://www.kaggle.com/datasets/thedevastator/australian-housing-data-1000-properties-sampled?resource=download) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f3bd6916",
            "metadata": {},
            "outputs": [],
            "source": [
                "file_path = \"data/\"\n",
                "file_name = \"RealEstateAU_NT_property.csv\"\n",
                "property_df = pd.read_csv(f\"{file_path}{file_name}\")\n",
                "property_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dbcc75be",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get the descriptive statiistics\n",
                "property_df.describe()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "55b6cff2",
            "metadata": {},
            "source": [
                "Are their any obvious correlations in the data?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6ac58b84",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "property_corr = property_df.corr()\n",
                "property_corr"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0f5ab5a6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a heatmap to visualise the correlations\n",
                "pc_fig = px.imshow(property_corr) \n",
                "pc_fig.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "b5088376",
            "metadata": {},
            "source": [
                "Parking is least correlated with price, so we can leave this out of the model and focus on those variables that correlate most with what we're interested in. Further for this model, we're only interested in numerical data, so we can leave out the property variable also (which is categorical)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b60a1537",
            "metadata": {},
            "outputs": [],
            "source": [
                "price_df = property_df[['price','bedrooms','bathrooms']]\n",
                "price_df"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "4d321179",
            "metadata": {},
            "source": [
                "Different visualisations can sometimes help better understand the relationships in the data. When looking for linear relationships, a pair plot can be helpful..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4e29ce30",
            "metadata": {},
            "outputs": [],
            "source": [
                "price_fig = px.scatter_matrix(price_df) # Create a pair plot to see the linearity of the variables\n",
                "price_fig.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "461f3956",
            "metadata": {},
            "source": [
                "To fit a linear regression model, we need to assign the dependent variable that we want to predict to the Y-axis, and draw the X-axis data from the independent variables."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "91cf75eb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Independent variables\n",
                "\n",
                "X_data = price_df[['bedrooms','bathrooms']]\n",
                "X_data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fd4ec264",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dependent variable\n",
                "\n",
                "y_data = price_df['price']\n",
                "y_data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2ca0601f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Break the current dataset into train and test datasets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, shuffle=True, train_size=0.8, random_state=99) # Train size determines the perceptage use for training the model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "129c2240",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a new linear regression model\n",
                "linear_model = LinearRegression() \n",
                "\n",
                "# Train the model with the train dataset\n",
                "linear_model.fit(X_train, y_train) \n",
                "\n",
                "# Predict using the testing dataset\n",
                "linear_predictions = linear_model.predict(X_test) \n",
                "linear_predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d9615ee6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate the error between the testing dataset the model's predictions\n",
                "linear_R2 = r2_score(y_test, linear_predictions) \n",
                "\n",
                "print(f'The model R squared score is: {linear_R2}')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "43d187b3",
            "metadata": {},
            "source": [
                "The **R-squared** is a coeficient between 0 and 1 that determine the quality of the model prediction. This number indicates the percentage of variance in the dependent variable that the independent variables explain. 0 means that the model's prediction is not explained at all by the independent variables, while 1 means that the model's prediction is 100% explained by the independent variables."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c3bb8992",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a chart to check the differences between what has been predicted and the real values\n",
                "y_test_fig_df = pd.DataFrame(y_test)\n",
                "linear_prediction_fig_df = pd.DataFrame(linear_predictions)\n",
                "linear_prediction_fig_df.columns = ['Predicted Price']\n",
                "linear_prediction_fig_df['Test Index'] = y_test_fig_df.index\n",
                "linear_prediction_fig_df.set_index('Test Index', inplace=True)\n",
                "linear_fig_df = linear_prediction_fig_df.join(y_test_fig_df)\n",
                "linear_fig = px.scatter(linear_fig_df)\n",
                "linear_fig.show()\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "83e35bee",
            "metadata": {},
            "source": [
                "#### Predict based on new values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d197bdd9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a dataframe with the new data\n",
                "new_df = pd.DataFrame([[4,5],[4,1],[2,3]],columns=['bedrooms','bathrooms'])\n",
                "\n",
                "new_df['lr_predict'] = linear_model.predict(new_df).astype(int)\n",
                "new_df.style.format('{:,}')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "71df6264",
            "metadata": {},
            "source": [
                "### Logistic Regression algorithm\n",
                "\n",
                "While linear regression is leveraged when dependent variables are continuous quantitative data, logistic regression can be used when the dependent variable is categorical. The most common type of logistic regression works with dependent variables with binary values such as \"true\" and \"false\" or \"yes\" and \"no.\" While both regression models seek to understand relationships between data inputs, logistic regression is mainly used to solve binary classification problems. Further information in [Logistic regression](https://en.wikipedia.org/wiki/Logistic_regression).\n",
                "\n",
                "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Exam_pass_logistic_curve.svg/800px-Exam_pass_logistic_curve.svg.png\" style=\"width: 50%\">\n",
                "\n",
                "#### Data\n",
                "\n",
                "For this example, we will use the same data as before, except instead of predicting price, we will see if we can predict as 'house' or 'apartment'. While this might not appear to be useful (as we would usually know what a property is), by predicting the relationships, we can identify anomalies. e.g. data that is different than what is expected. This might help with tasks such as setting an appropriate price for sale.\n",
                "\n",
                "First it is important to understand the balance between values of the dependent variable"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "16f1bd73",
            "metadata": {},
            "outputs": [],
            "source": [
                "property_df.groupby('property').count()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "0fdeba1a",
            "metadata": {},
            "source": [
                "These are reasonably balanced so it should not be a problem in the analysis. Skewed data can be problematic as there may not be sufficient data for one dependent value to train an effective model."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "a6e72918",
            "metadata": {},
            "source": [
                "As with the linear regression, we need to create separate datasets for dependent, independent, and training and test."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9295fb12",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Independent variables data - start with all options\n",
                "X_data = property_df[['price','bedrooms','bathrooms','parking']] \n",
                "\n",
                "# Dependent variable data\n",
                "y_data = property_df['property']=='House' # convert to True False "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "098d6216",
            "metadata": {},
            "outputs": [],
            "source": [
                "y_data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fc4f32e4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Break the current dataset into train and test datasets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, shuffle=True, train_size=0.8, random_state=99) # Train size determines the perceptage use for training the model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "64dfadc7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check the class balance\n",
                "y_train.value_counts(normalize=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "3ccec52a",
            "metadata": {},
            "source": [
                "There is a small class inbalance in the variable that we are going to predict. Therefore, the model is likely to predict towards 'No' just because the biased data rather than the independent variables. In any classification model such as logistic regression, decision trees, etc. The class balance need to be considered."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b94aa394",
            "metadata": {},
            "outputs": [],
            "source": [
                "logistic_model = LogisticRegression(class_weight={False: 0.53, True: 0.47})"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "4d3859f7",
            "metadata": {},
            "source": [
                "Additionally, it is a common practice to scale the date to have a better model. To scale the data wwe are going to use standardization that scale the data to have a mean of 0 and a standard deviation of 1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0b0361cf",
            "metadata": {},
            "outputs": [],
            "source": [
                "scale = StandardScaler()\n",
                "X_train = scale.fit_transform(X_train)\n",
                "X_test = scale.transform(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d80b760b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fit the model to the training dataset\n",
                "logistic_model.fit(X_train, y_train)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "4bab0e7f",
            "metadata": {},
            "source": [
                "To evaluate the model the most common method is the confusion matrix."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d358b596",
            "metadata": {},
            "outputs": [],
            "source": [
                "logistic_prediction = logistic_model.predict(X_test) # Use the model to predict based on the testing dataset\n",
                "cm = confusion_matrix(y_test, logistic_prediction) # Compare the model's prediction against the true value in the testing dataset\n",
                "cm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3c29ebbd",
            "metadata": {},
            "outputs": [],
            "source": [
                "cm_fig = px.imshow(cm, labels={'x': 'Predicted label', 'y': 'Actual label'})\n",
                "cm_fig.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "18980c6d",
            "metadata": {},
            "outputs": [],
            "source": [
                "report = classification_report(y_test, logistic_prediction) # Get further evaluation metrics\n",
                "print(report)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "632653e3",
            "metadata": {},
            "source": [
                "#### Precision and Recall\n",
                "\n",
                "Precision and recall are important factors in understanding the effectiveness of a model.\n",
                "\n",
                "- Precision: What proportion of positive identifications was actually correct? \n",
                "- Recall: What proportion of relevant elements were identified correctly?\n",
                "\n",
                "See this[Google developers page](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall) for a helpful explanation.\n",
                "See this [Wikipedia page](https://en.wikipedia.org/wiki/Precision_and_recall) for more information.\n",
                "\n",
                "\n",
                "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png\" width=\"30%\">\n",
                "\n",
                "[Image source](https://commons.wikimedia.org/wiki/File:Precisionrecall.svg)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fb4e78fe",
            "metadata": {},
            "source": [
                "Additional explanations can be found in this [Medium Article](https://medium.com/@nirajan.acharya666/understanding-precision-recall-f1-score-and-support-in-machine-learning-evaluation-7ec935e8512e#:~:text=F1%2Dscore%20is%20useful%20when,of%20instances%20in%20each%20class.)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "698bde5f",
            "metadata": {},
            "source": [
                "### Predict a classification on new data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d2bb52f1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a dataframe with the new data\n",
                "new_df = pd.DataFrame([[400000,2,1,1],[900000,4,2,2]],columns=['price','bedrooms','bathrooms','parking'])\n",
                "# Scale the same as the model data\n",
                "X_new = scale.transform(new_df)\n",
                "# Predict classification\n",
                "new_prediction = logistic_model.predict(X_new)\n",
                "new_df['logist_house_pred'] = new_prediction\n",
                "new_df"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "49659b6d",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "### Further exploration\n",
                "\n",
                "Take a look at other ML approaches available using the [Scikit-learn library](https://scikit-learn.org/stable/).\n",
                "\n",
                "Try different approaches with your own data."
            ]
        }
    ],
    "metadata": {
        "creation_period": "",
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.10"
        },
        "nb_name": "template",
        "qut": {
            "creation_period": "2023_sem1",
            "nb_name": "C4-MachineLearning-studio",
            "unit_code": "IFN619"
        },
        "unit_code": ""
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
