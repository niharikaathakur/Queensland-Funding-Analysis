{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca249a0-2ebd-4c90-91b6-16d6351ad606",
   "metadata": {
    "cell_name": "header_cell"
   },
   "source": [
    "<div style=\"background:#E9FFF6; color:#440404; padding:8px; border-radius: 4px; text-align: center; font-weight: 500;\">IFN619 - Data Analytics for Strategic Decision Makers (2024 Sem 1)</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bc6489c-6c2c-4aec-8962-63884b4984ed",
   "metadata": {
    "cell_name": "title_cell"
   },
   "source": [
    "# IFN619 :: C1-UnstructuredAnalytics\n",
    "\n",
    "For this session, the focus will be on analysis of unstructured text. However, the thinking required is similar to approaches to analysing images, video, sound and other unstructured data. Primarily, the analysis is based on the notion that there are useful patterns in the unstructured data which can be obtained mathematically. By converting the data to a mathematical structure, various algorithms can be applied to the structure with the aim of identifying patterns. \n",
    "\n",
    "In the case of the `topic modelling` approaches below, many of the techniques are *probabilistic* - that is they mathematically identify the *likelihood* that a feature might be important. Thus, they are never 100% accurate, and their use needs to be mediated by a more pragmatic *useful or not* approach, rather than *right or wrong*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "import pandas as pd\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d3afe99",
   "metadata": {},
   "source": [
    "### Accessing the data via The Guardian API\n",
    "\n",
    "See the `Accessing_the_Guardian_API.ipynb` notebook file for details on getting the data. **Note:** This approach may be used for additional data for Assignment 2.\n",
    "\n",
    "### Read in pre-saved data\n",
    "\n",
    "To save time, we're loading in pre-saved data that was fetched using the Guardian API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a95d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data - articles from The Guardian about the war in Ukraine\n",
    "file_path = \"data/\"\n",
    "file_name = ???\n",
    "\n",
    "with open(f\"{file_path}{file_name}\",'r', encoding='utf-8') as fp:\n",
    "    articles = json.load(fp)\n",
    "\n",
    "print(f\"Loaded {len(articles)} articles from {file_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8010b4b5",
   "metadata": {},
   "source": [
    "Each dictionary entry includes the *title [date]* as `key` and the *body text* from the article as `value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310db6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "article1 = list(articles.items())[0]\n",
    "print(\"Key:\",article1[0])\n",
    "print(\"Value:\",article1[1][:300],\"...\") # Just show first 300 characters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74fac8ad",
   "metadata": {},
   "source": [
    "So the values gives us a list of documents that we can analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of documents\n",
    "documents = list(articles.values())\n",
    "\n",
    "# View first 400 characters of the 1st document\n",
    "documents[0][:???]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "113510df",
   "metadata": {},
   "source": [
    "### Term Count \n",
    "\n",
    "**Finding important terms by the frequency of their occurance**\n",
    "\n",
    "Using `CountVectorizer` create a `vector` for each document where the dimensionality of the vector is the `vocabulary` (all terms in the collection), and the value of each component is the number of times that the `term` occurs in the document.\n",
    "\n",
    "All of these analyses, approach the document as a [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) model. In this approach, the order of the words don't matter. A popular approach that takes into account order is [Word embedding](https://en.wikipedia.org/wiki/Word_embedding). This session does not explore word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only count terms that in maximum of 75% of documents, and a minimum of 2 documents. \n",
    "# Count a maximum of 10000 terms, and remove common english stop words\n",
    "count_vectorizer = CountVectorizer(max_df=???,min_df=???,max_features=???,stop_words=\"english\")\n",
    "count_dt_matrix = count_vectorizer.fit_transform(articles.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c0d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the vector for the first document\n",
    "doc001_vector = count_dt_matrix.toarray()[???]\n",
    "doc001_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8add7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 1000 terms identified during the vectorization process\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa088ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at how the counts match up to the terms (for the 1st doc)\n",
    "doc001_term_counts = list(zip(feature_names,???))\n",
    "doc001_term_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d7eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the vocabulary which shows the total counts for whole collection\n",
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bf88710",
   "metadata": {},
   "source": [
    "#### Display matrix in dataframe\n",
    "\n",
    "Take the term count matrix and display in a dataframe to make visible the structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7354d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with the matrix - use titles for the index and terms for the columns\n",
    "count_df = pd.DataFrame(???.toarray(), index=articles.???, columns=???)\n",
    "count_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2974af8e",
   "metadata": {},
   "source": [
    "By selecting a row from the dataframe and sorting the values (counts), we can identify the top 10 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b719f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 5 random articles\n",
    "samples = random.sample(range(0,len(terms_df)),5)\n",
    "\n",
    "for sample in samples:\n",
    "    doc = count_df.iloc[???]\n",
    "    top_terms = dict(count_df.iloc[sample].sort_values(ascending=False).head(???))\n",
    "    print(f\"[{sample}] {doc.name}\")\n",
    "    print(\"\\t- Top terms:\",top_terms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e2378cc",
   "metadata": {},
   "source": [
    "#### Create a top10 terms dataframe\n",
    "\n",
    "Using the index from the documents, create a dataframe that can hold the top10 terms for each document. We also include columns for our other analysis (tfidf, lda, nmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58edcb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to hold top terms for each analysis type\n",
    "terms_df = pd.DataFrame(index=count_df.???,columns=['count','tfidf','lda','nmf'])\n",
    "terms_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c139dd5b",
   "metadata": {},
   "source": [
    "Populate the count column with data created by the count vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b43700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each doc, get the 10 columns with the largest counts\n",
    "for idx in terms_df.index:\n",
    "    counts = dict(???.loc[idx].sort_values(ascending=False).head(???))\n",
    "    #print(counts)\n",
    "    terms_df.at[idx,'count'] = list(counts.keys()) # Just the list of terms\n",
    "\n",
    "terms_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf4663b5",
   "metadata": {},
   "source": [
    "### Term Frequency / Inverse Document Frequency (TF/IDF)\n",
    "\n",
    "**Finding terms that are very common in a document, but less common in the whole collection**\n",
    "\n",
    "The [TF/IDF](https://en.wikipedia.org/wiki/Tfâ€“idf) algorithm takes the term frequencies for a document and divides them by the frequencies of the terms in the whole collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c3b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only count terms that in maximum of 75% of documents, and a minimum of 2 documents. \n",
    "# Count a maximum of 10000 terms, and remove common english stop words\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=???, min_df=???, max_features=???, stop_words=\"english\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56935be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the document vectors\n",
    "tfidf_dt_matrix = tfidf_vectorizer.fit_transform(???)\n",
    "\n",
    "# Display the vector for the first document\n",
    "tfidf_dt_matrix.toarray()[???]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd5c6b13",
   "metadata": {},
   "source": [
    "#### Display matrix in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa6b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(???.toarray(), index=articles.keys(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b8893fd",
   "metadata": {},
   "source": [
    "#### Update the terms matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6eb661",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in terms_df.index:\n",
    "    tfidf = dict(tfidf_df.loc[idx].sort_values(ascending=False).head(???))\n",
    "    #print(counts)\n",
    "    terms_df.at[idx,'tfidf'] = list(tfidf.keys()) \n",
    "\n",
    "terms_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f1b1b9f",
   "metadata": {},
   "source": [
    "#### Compare approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c24a660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 5 random articles\n",
    "samples = random.sample(range(0,len(terms_df)),5)\n",
    "\n",
    "for sample in samples:\n",
    "    doc = terms_df.iloc[sample]\n",
    "    print(f\"[{sample}] {doc.name}\")\n",
    "    print(\"\\t- Counts:\\t\",doc['count'])\n",
    "    print(\"\\t- TFIDF:\\t\",doc['tfidf'])\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca19dd67",
   "metadata": {},
   "source": [
    "### Topic modelling with Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "[LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) is an algorithm for obtaining *topics* (a list of terms) from a document-term matrix. It is a generative probabilistic approach to *decomposition* of the document-term matrix into 2 factor matrices: document-topic and topic-term.\n",
    "\n",
    "![img](https://editor.analyticsvidhya.com/uploads/26864dtm.JPG)\n",
    "\n",
    "*Source: [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/06/part-2-topic-modeling-and-latent-dirichlet-allocation-lda-using-gensim-and-sklearn/)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37d90824",
   "metadata": {},
   "source": [
    "The LDA model requires the number of topics to be set in advance. As it is a generative model, it also runs over a number of iterations. These values usually need to be experimented with to obtain quality topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of topics\n",
    "num_topics = 20\n",
    "# Set max number of iteractions\n",
    "max_iterations = 10\n",
    "\n",
    "# Create the model\n",
    "lda_model = LatentDirichletAllocation(n_components=???,max_iter=???,learning_method='online')\n",
    "\n",
    "# Fit the model to the data, and use the model to transform the data (do the decomposition)\n",
    "doc_topic_matrix = lda_model.fit_transform(???)\n",
    "\n",
    "# Obtain the topics\n",
    "topic_term_matrix = lda_model.components_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e32c2dc",
   "metadata": {},
   "source": [
    "#### View the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd1b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topics and their terms\n",
    "lda_topic_dict = {}\n",
    "for index, topic in enumerate(???):\n",
    "    zipped = zip(feature_names, topic)\n",
    "    top_terms=dict(sorted(zipped, key = lambda t: t[1], reverse=True)[:10])\n",
    "    print(top_terms)\n",
    "    top_terms_list= {key : round(top_terms[key], 4) for key in top_terms.keys()}\n",
    "    lda_topic_dict[f\"topic_{index}\"] = top_terms_list\n",
    "\n",
    "# Print the topics with their terms    \n",
    "for k,v in lda_topic_dict.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8bc732c",
   "metadata": {},
   "source": [
    "#### List of topics for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e826429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix[200]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b770bf4a",
   "metadata": {},
   "source": [
    "#### Update the terms matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6047f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,topic in enumerate(???):\n",
    "    topic_num = topic.argmax()\n",
    "    top_topic = lda_topic_dict[f\"topic_{topic_num}\"]\n",
    "    terms_df['lda'].iloc[idx] = list(top_topic.keys())\n",
    "\n",
    "terms_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4c77cee",
   "metadata": {},
   "source": [
    "#### Compare approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa00333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 5 random articles\n",
    "samples = random.sample(range(0,len(terms_df)),5)\n",
    "\n",
    "for sample in samples:\n",
    "    doc = terms_df.iloc[sample]\n",
    "    print(f\"[{sample}] {doc.name}\")\n",
    "    print(\"\\t- Counts:\\t\",doc['count'])\n",
    "    print(\"\\t- TFIDF:\\t\",doc['tfidf'])\n",
    "    print(\"\\t- LDA:\\t\\t\",doc['lda'])\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf2ede50",
   "metadata": {},
   "source": [
    "### Topic modelling with Non-negative Matrix Factorisation (NMF)\n",
    "\n",
    "\n",
    "[NMF](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) is a different algorithm for obtaining *topics* (a list of terms) from a document-term matrix. It also factorises the document-term matrix into 2 factor matrices: document-topic and topic-term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of topics\n",
    "num_topics = 20\n",
    "\n",
    "# Create the model\n",
    "nmf_model = NMF(n_components=???,init='random',beta_loss='frobenius')\n",
    "\n",
    "# Fit the model to the data and use it to transform the data\n",
    "doc_topic_nmf = nmf_model.fit_transform(???)\n",
    "\n",
    "topic_term_nmf = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284934af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topics and their terms\n",
    "nmf_topic_dict = {}\n",
    "for index, topic in enumerate(???):\n",
    "    zipped = zip(feature_names, topic)\n",
    "    top_terms=dict(sorted(zipped, key = lambda t: t[1], reverse=True)[:10])\n",
    "    #print(top_terms)\n",
    "    top_terms_list= {key : round(top_terms[key], 4) for key in top_terms.keys()}\n",
    "    nmf_topic_dict[f\"topic_{index}\"] = top_terms_list\n",
    "\n",
    "# Print the topics with their terms    \n",
    "for k,v in nmf_topic_dict.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f9218b0",
   "metadata": {},
   "source": [
    "#### Update the terms matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c2c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,topic in enumerate(???):\n",
    "    topic_num = topic.argmax()\n",
    "    top_topic = lda_topic_dict[f\"topic_{topic_num}\"]\n",
    "    terms_df['nmf'].iloc[idx] = list(top_topic.keys())\n",
    "\n",
    "terms_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48aed516",
   "metadata": {},
   "source": [
    "### Compare approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359038ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 5 random articles\n",
    "samples = random.sample(range(0,len(terms_df)),5)\n",
    "\n",
    "for sample in samples:\n",
    "    doc = terms_df.iloc[sample]\n",
    "    print(f\"[{sample}] {doc.name}\")\n",
    "    print(\"\\t- Counts:\\t\",doc['count'])\n",
    "    print(\"\\t- TFIDF:\\t\",doc['tfidf'])\n",
    "    print(\"\\t- LDA:\\t\\t\",doc['lda'])\n",
    "    print(\"\\t- NMF:\\t\\t\",doc['nmf'])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "creation_period": "",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "nb_name": "template",
  "qut": {
   "creation_period": "2023_sem1",
   "nb_name": "C1-UnstructuredAnalytics",
   "unit_code": "IFN619"
  },
  "unit_code": ""
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
